# stavish_bot
creating a slackbot that scrapes UMD dashboards

MEMO:

I think this bot could be better but at this point I've done about 85% of what I wanted to get done. This bot uses BeautifulSoup to look at the html of a website and determine what has changed. To do this I first direct the bot to look at a div that covers pretty much all of the site that I'm interested in. Then, I direct the bot to specific divs and classes and ask it to give me the text that exists in those divs or classes. I get an email when things are updated (I tested this with another site that's updated regularly that was recommended for this purpose when I was looking up information on BeautifulSoup) and I don't get an email if nothing is updated. That is my preffered schedule for updates.

Getting the output to contain only text and no html was a learning experience because the way I thought I was supposed to extract text only doesn't work with loops. I learned I had to write a loop that goes through all the divs and classes I want it to go through and then ask it to extract only text after the fact. This also got slightly complicated when divs I was looping through contained multiple things. I learned to loop through a div but then only find all of the <p> tags (for example) within those loops. That complicated the text extraction, but I eventually got the hang of it.

I'm still struggling with the 'actions' column of my csv. This is because the website has some information in buttons and figuring that out how to scrape from a pop-up window has been tough for me. However, if those sections are updated, I'll still be notified because the 'last updated' column will be updated with the date anything on the website was changed. But I still want to get the 'actions' column working because I want documentation of how those things changed, not just the fact that they changed.

I am a little frustrated with myself at how much time I spent experimenting with other libraries that didn't pan out. I wish I had prioritized BeautifulSoup early on mainly because there is so much documentation online about it that I was generally able to look up my questions and get pointed in the right direction.

I used ChatGPT a few times throughout this process. By far the most helpful thing was pasting my current code into ChatGPT and asking it to properly indent everything for me. That saved me so much frustration. I also would input my code and whatever error I got and asked ChatGPT what the errors meant. This is really helpful for me and I like that I can ask questions about the specifics of the error. Again, this was not always correct and ChatGPT seldom fully solved my problem, but it often pointed me in the right direction. If I was stuck I would compare my question and ChatGPTs answer with StackOverflow posts and see what matched up and what didn't which was actually sorta fun. 

Right now this data is stored in a CSV, which I think is fine for my own purposes. I'm unsure on how I could improve storage for broader use, especially because I'd like to keep old versions of the csv files to see how often things are updated and such. Maybe some storage format that has a time element. If this bot were to accept input from users I could see users asking basic questions like 'how many demands are currently marked as sustained commitments?' or 'When was the last time this website was updated?' or 'Which demand has been most frequently updated? Least frequently?'. I think it would also be interesting to do text analysis on this page, espcially for the 'actions' section because there's a lot of words like 'continuing', 'participating', and 'discussing' and other words that don't always mean anything significant. Would make a good bull-shit-o-meter.
